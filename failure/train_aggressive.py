import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import argparse
import os
from evaluate import evaluate_in_memory
from dataset import ASCADv2Dataset
from model import EstraNet
from scoop import SCOOP 

def set_learning_rate(optimizer, new_lr):
    for param_group in optimizer.param_groups:
        param_group['lr'] = new_lr

def train(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”¥ Starting AGGRESSIVE Training on {device}")
    
    model = EstraNet(d_model=args.d_model, n_head=args.n_head, n_layers=args.n_layer).to(device)
    
    # åˆå§‹è®¾ç½®
    # æ³¨æ„ï¼šæˆ‘ä»¬æ‰‹åŠ¨æ§åˆ¶å­¦ä¹ ç‡ï¼Œæ‰€ä»¥è¿™é‡Œåˆå§‹åŒ–ç»™ä¸€ä¸ªæå°å€¼ï¼Œåé¢æ‰‹åŠ¨Warmup
    optimizer = SCOOP(model.parameters(), lr=1e-8, rho=0.96)
    
    if not os.path.exists(args.result_path): os.makedirs(args.result_path)
    if not os.path.exists(args.checkpoint_dir): os.makedirs(args.checkpoint_dir)

    dataset = ASCADv2Dataset(args.data_path, split='train', input_len=args.input_length)
    loader = DataLoader(dataset, batch_size=args.train_batch_size, shuffle=True, num_workers=4, pin_memory=True)
    criterion = nn.CrossEntropyLoss()
    
    # --- âš™ï¸ æ¿€è¿›ç­–ç•¥é…ç½® ---
    TARGET_LR = args.learning_rate  # æ¯”å¦‚ 3e-4
    WARMUP_EPOCHS = 10              # è¶…é•¿çƒ­èº«ï¼š10ä¸ªEpoch
    steps_per_epoch = len(loader)
    warmup_steps = steps_per_epoch * WARMUP_EPOCHS
    
    # çŠ¶æ€æ ‡è®°
    has_decayed = False             # æ˜¯å¦å·²ç»è§¦å‘è¿‡â€œ10å€å‡é€Ÿâ€
    
    log_file = os.path.join(args.result_path, "train_log_aggressive.csv")
    if not os.path.exists(log_file):
        with open(log_file, "w") as f: f.write("Epoch,TrainLoss,TestLoss,Rank,LR\n")

    hessian_freq = 10
    epochs = args.train_steps // len(loader) + 1
    global_step = 0

    print(f"Plan: Warmup to {TARGET_LR} for {WARMUP_EPOCHS} epochs.")
    print(f"Trigger: If Rank < 100, LR will drop to {TARGET_LR / 10:.1e}")

    for epoch in range(epochs):
        model.train()
        pbar = tqdm(loader, desc=f"Epoch {epoch}")
        total_loss = 0
        count = 0
        
        for i, (data, target) in enumerate(pbar):
            data, target = data.to(device), target.to(device)
            update_h = (i % hessian_freq == 0)
            
            # --- 1. æ‰‹åŠ¨ Warmup é€»è¾‘ ---
            if global_step < warmup_steps:
                # çº¿æ€§å¢åŠ 
                warmup_lr = TARGET_LR * (global_step / warmup_steps)
                set_learning_rate(optimizer, warmup_lr)
            # Warmup ç»“æŸåï¼Œä¿æŒ TARGET_LRï¼Œç›´åˆ°è§¦å‘ decay
            
            optimizer.zero_grad()
            out = model(data)
            loss = criterion(out, target)
            loss.backward(create_graph=update_h)
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)
            
            if update_h: optimizer.hutchinson_hessian()
            optimizer.step()
            if update_h: optimizer.zero_grad()
            
            total_loss += loss.item()
            count += 1
            global_step += 1
            
            current_lr = optimizer.param_groups[0]['lr']
            pbar.set_postfix({'loss': f"{loss.item():.4f}", 'lr': f"{current_lr:.6f}"})
            
            if global_step >= args.train_steps: break

        # --- éªŒè¯é˜¶æ®µ ---
        # å¼ºåˆ¶ä½¿ç”¨ 10000 æ¡ï¼Œç¡®ä¿ Rank æ˜¯çœŸçš„
        rank, test_loss = evaluate_in_memory(model, args.data_path, n_test=10000, device=device)
        
        avg_train_loss = total_loss / count
        current_lr = optimizer.param_groups[0]['lr']
        print(f"\nEpoch {epoch}: Train {avg_train_loss:.4f} | Test {test_loss:.4f} | Rank {rank} | LR {current_lr:.6f}")
        
        # --- âš™ï¸ 2. æ™ºèƒ½åˆ¹è½¦é€»è¾‘ ---
        # å¦‚æœ Rank ç¡®å®é™åˆ°äº† 100 ä»¥ä¸‹ï¼Œå¹¶ä¸”è¿˜æ²¡æœ‰å‡é€Ÿè¿‡ï¼Œä¸” Warmup å·²ç»ç»“æŸ
        if rank < 100 and not has_decayed and global_step > warmup_steps:
            print(f"ğŸš€ SUCCESS! Rank {rank} < 100 detected.")
            print(f"ğŸ“‰ Triggering 10x LR Decay: {current_lr:.1e} -> {current_lr * 0.1:.1e}")
            
            # æ°¸ä¹…å‡é€Ÿ
            TARGET_LR = TARGET_LR * 0.1 
            set_learning_rate(optimizer, TARGET_LR)
            has_decayed = True
            
            # ä¿å­˜è¿™ä¸ªå…³é”®æ—¶åˆ»çš„æ¨¡å‹
            torch.save(model.state_dict(), os.path.join(args.checkpoint_dir, f"estranet_breakthrough_rank{rank}.pth"))

        with open(log_file, "a") as f:
            f.write(f"{epoch},{avg_train_loss:.4f},{test_loss:.4f},{rank},{current_lr:.6f}\n")
            
        torch.save(model.state_dict(), os.path.join(args.checkpoint_dir, "estranet_latest.pth"))
        if rank < 100 or epoch % 10 == 0:
            torch.save(model.state_dict(), os.path.join(args.checkpoint_dir, f"estranet_epoch_{epoch}.pth"))
        
        if global_step >= args.train_steps: break

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_path", type=str, required=True)
    parser.add_argument("--checkpoint_dir", type=str, default="./checkpoints")
    parser.add_argument("--result_path", type=str, default="./results")
    parser.add_argument("--learning_rate", type=float, default=3e-4) # é»˜è®¤æ¿€è¿› LR
    parser.add_argument("--input_length", type=int, default=15000)
    parser.add_argument("--train_batch_size", type=int, default=64)
    parser.add_argument("--train_steps", type=int, default=400000)
    parser.add_argument("--d_model", type=int, default=128)
    parser.add_argument("--n_layer", type=int, default=2)
    parser.add_argument("--n_head", type=int, default=8)
    parser.add_argument("--clip", type=float, default=5.0)
    
    args, unknown = parser.parse_known_args()
    train(args)