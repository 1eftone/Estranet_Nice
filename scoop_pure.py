import torch
from torch.optim import Optimizer

class SCOOP(Optimizer):
    def __init__(self, params, lr=1e-4, betas=(0.5, 0.999), rho=0.96, epsilon=1e-5, weight_decay=1e-4):
        """
        Ultimate SCOOP: Low Inertia + Safety Clamp + Robust Hessian
        """
        defaults = dict(lr=lr, betas=betas, rho=rho, epsilon=epsilon, weight_decay=weight_decay)
        super(SCOOP, self).__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None):
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                
                grad = p.grad
                state = self.state[p]

                # åˆå§‹åŒ–
                if len(state) == 0 or 'exp_avg' not in state:
                    state['step'] = 0
                    state['exp_avg'] = torch.zeros_like(p)
                    state['hessian'] = torch.ones_like(p) # å®‰å…¨èµ·è§åˆå§‹è®¾ä¸º 1

                exp_avg = state['exp_avg']
                hessian = state['hessian']
                beta1, _ = group['betas']
                state['step'] += 1

                # 1. Weight Decay (é˜²æ­¢èººå¹³)
                if group['weight_decay'] != 0:
                    grad = grad.add(p, alpha=group['weight_decay'])

                # 2. Momentum (ä½æƒ¯æ€§ 0.5)
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)

                # 3. Apply Curvature
                curvature = hessian.abs()
                
                # ğŸ”¥ å®‰å…¨é’³ä½ï¼šé˜²æ­¢æ—§æœåŠ¡å™¨ç²¾åº¦ä¸è¶³å¯¼è‡´é™¤é›¶çˆ†ç‚¸
                curvature = torch.clamp(curvature, min=1e-5)
                
                denom = curvature.add_(group['epsilon'])
                
                # Update
                p.addcdiv_(exp_avg, denom, value=-group['lr'])

        return loss

    def hutchinson_hessian(self, num_samples=1):
        """
        Hutchinson's method with robust filtering for detached gradients.
        """
        params = []
        groups = []
        for group in self.param_groups:
            for p in group['params']:
                # ğŸ”¥ ä¸¥æ ¼è¿‡æ»¤ï¼šå¿…é¡»æœ‰ grad ä¸” grad å¿…é¡»æœ‰è®¡ç®—å›¾
                if p.requires_grad and p.grad is not None and p.grad.requires_grad:
                    params.append(p)
                    groups.append(group)

        # å¦‚æœæ²¡æœ‰å¯æ±‚äºŒé˜¶å¯¼çš„å‚æ•°ï¼Œç›´æ¥è¿”å›ï¼Œé˜²æ­¢ crash
        if not params:
            return

        grads = [p.grad for p in params]

        for i in range(num_samples):
            # Rademacher distribution
            vs = [torch.randint_like(p, high=2) * 2 - 1 for p in params]
            
            # Matrix-Vector Product
            grad_dot_v = sum([torch.sum(g * v) for g, v in zip(grads, vs)])
            
            # ğŸ”¥ å…è®¸æœªä½¿ç”¨çš„æ¢¯åº¦ (allow_unused=True)
            hvs = torch.autograd.grad(
                grad_dot_v, params, 
                retain_graph=(i < num_samples - 1), 
                only_inputs=True,
                allow_unused=True 
            )
            
            for p, v, hv, group in zip(params, vs, hvs, groups):
                if hv is None:
                    # å¦‚æœäºŒé˜¶å¯¼ä¸å­˜åœ¨ï¼Œè§†ä¸º 0
                    current_curvature = torch.zeros_like(p)
                else:
                    current_curvature = v * hv
                
                state = self.state[p]
                if 'hessian' not in state:
                    state['hessian'] = torch.ones_like(p)
                
                # å¹³æ»‘æ›´æ–° Hessian
                state['hessian'].mul_(group['rho']).add_(current_curvature, alpha=1 - group['rho'])