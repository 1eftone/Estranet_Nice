import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import argparse
import os
import sys
import numpy as np
import random
import torch.autograd as autograd

# ç¡®ä¿è¿™äº›æ¨¡å—å­˜åœ¨
from evaluate import evaluate_in_memory
from dataset import ASCADv2Dataset
from model import EstraNet

# ğŸ”¥ 0. å›ºå®šéšæœºç§å­
def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# ğŸ”¥ 1. å¼ºåˆ¶ç¦ç”¨ PyTorch 2.0+ é«˜æ•ˆ Attention (ä¸ºäº†äºŒé˜¶å¯¼æ•°è®¡ç®—)
try:
    torch.backends.cuda.enable_flash_sdp(False)
    torch.backends.cuda.enable_mem_efficient_sdp(False)
    torch.backends.cuda.enable_math_sdp(True)
except AttributeError:
    pass

def set_lr(optimizer, new_lr):
    for param_group in optimizer.param_groups:
        param_group['lr'] = new_lr

# ğŸ”¥ æ‰‹åŠ¨è®¡ç®— Hessian å‘é‡ç§¯ (HVP) å¹¶åŠ åˆ°æ¢¯åº¦ä¸Š
# è¿™å°±æ˜¯ SCOOP çš„æ ¸å¿ƒé€»è¾‘ï¼Œæˆ‘ä»¬æŠŠå®ƒè§£è€¦å‡ºæ¥ï¼Œä¸ºäº†èƒ½éšæ—¶å…³é—­
def add_hessian_regularization(model, loss, optimizer, rho=0.96):
    params = [p for p in model.parameters() if p.requires_grad]
    
    # 1. è®¡ç®—ä¸€é˜¶æ¢¯åº¦ (Grad)
    grads = autograd.grad(loss, params, create_graph=True, retain_graph=True)
    
    # 2. ç”Ÿæˆ Hutchinson éšæœºå‘é‡ (v)
    v = [torch.randint_like(p, high=2) * 2 - 1 for p in params]
    
    # 3. è®¡ç®— Hv (Hessian-Vector Product)
    # H*v = grad(grad(loss)*v)
    grad_v = sum([torch.sum(g * s) for g, s in zip(grads, v)])
    Hv = autograd.grad(grad_v, params, retain_graph=False)
    
    # 4. å°†å¹³æ»‘åçš„æ¢¯åº¦æ›´æ–°åˆ° p.grad
    # g_new = g + rho * (Hv * v - g) / (1 + rho) 
    # (ç®€åŒ–çš„ SCOOP æ›´æ–°è§„åˆ™)
    
    with torch.no_grad():
        for i, p in enumerate(params):
            if p.grad is None: continue
            
            # SCOOP æ ¸å¿ƒå…¬å¼: æ ¡æ­£æ¢¯åº¦
            # g_scoop = g + (rho * (Hv * v) - rho * g)
            # è¿™é‡Œæˆ‘ä»¬åšä¸€ä¸ªç®€å•çš„åŠ æƒèåˆï¼Œæ•ˆæœç±»ä¼¼
            p.grad.add_(Hv[i] * v[i], alpha=rho)
            p.grad.div_(1 + rho)

def train(args):
    seed_everything(45)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nğŸ”¥ STRATEGY: AdamW + SCOOP (Switching to Pure AdamW at Rank 15) on {device}")
    
    model = EstraNet(d_model=args.d_model, n_head=args.n_head, n_layers=args.n_layer).to(device)
    
    # ğŸ”¥ 2. ä½¿ç”¨åŸç”Ÿ AdamW
    # AdamW å¯¹å™ªå£°çš„é²æ£’æ€§æ¯” SGD å¼ºå¾ˆå¤šï¼Œé€‚åˆ 10k è¿™ç§å°æ ·æœ¬
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=args.learning_rate, 
        weight_decay=1e-2, # AdamW éœ€è¦ç¨å¤§çš„ WD
        betas=(0.9, 0.999)
    )
    
    if not os.path.exists(args.result_path): os.makedirs(args.result_path)
    if not os.path.exists(args.checkpoint_dir): os.makedirs(args.checkpoint_dir)

    dataset = ASCADv2Dataset(args.data_path, split='train', input_len=args.input_length)
    loader = DataLoader(dataset, batch_size=args.train_batch_size, shuffle=True, num_workers=4, pin_memory=True)
    criterion = nn.CrossEntropyLoss()
    
    best_rank = float('inf')
    
    # --- ç­–ç•¥é…ç½® ---
    
    # é˜¶æ®µ 1: Adam + SCOOP (å¿«é€Ÿä¸‹é™)
    # ä½¿ç”¨ Hessian å¯¼èˆªï¼Œé˜²æ­¢ Adam è¿‡æ—©é™·å…¥å±€éƒ¨æœ€ä¼˜
    HESSIAN_ENABLED = True 
    HESSIAN_FREQ = 2
    
    # é˜¶æ®µ 2: Pure Adam (è„‘åˆ‡é™¤)
    # Rank < 15 æ—¶è§¦å‘
    TRIGGER_PURE_ADAM_RANK = 0
    PURE_ADAM_LR = 1e-4  # Adam çš„ 5e-6 çº¦ç­‰äº SGD çš„ 1e-6ï¼Œéå¸¸ç»†è…»
    pure_adam_triggered = False
    
    steps_per_epoch = len(loader)
    warmup_epochs = 1 # Adam ä¸éœ€è¦å¤ªé•¿çš„ Warmup
    warmup_steps = steps_per_epoch * warmup_epochs
    
    log_file = os.path.join(args.result_path, "train_log_adam_scoop.csv")
    if not os.path.exists(log_file):
        with open(log_file, "w") as f: f.write("Epoch,TrainLoss,TestLoss,Rank,LR,Mode\n")

    epochs = args.train_steps // steps_per_epoch + 1
    global_step = 0

    print(f"âš™ï¸ Config: AdamW LR={args.learning_rate} | Pure Adam Trigger < {TRIGGER_PURE_ADAM_RANK} (LR={PURE_ADAM_LR})\n")

    for epoch in range(epochs):
        model.train()
        pbar = tqdm(loader, desc=f"Epoch {epoch}")
        total_loss = 0
        count = 0
        
        for i, (data, target) in enumerate(pbar):
            data, target = data.to(device), target.to(device)
            
            # åªæœ‰åœ¨æœªè§¦å‘çº¯ Adam æ¨¡å¼ï¼Œä¸”ç¬¦åˆé¢‘ç‡æ—¶ï¼Œæ‰è®¡ç®— Hessian
            update_h = HESSIAN_ENABLED and (not pure_adam_triggered) and (i % HESSIAN_FREQ == 0)
            
            # Warmup
            if not pure_adam_triggered and global_step < warmup_steps:
                lr_scale = float(global_step) / float(max(1, warmup_steps))
                set_lr(optimizer, args.learning_rate * lr_scale)
            
            optimizer.zero_grad(set_to_none=True)
            
            out = model(data)
            loss = criterion(out, target)
            
            # ğŸ”¥ ä¿®å¤ç‚¹ 1: å…ˆæŠŠæ•°å€¼å­˜ä¸‹æ¥ï¼
            loss_val = loss.item()

            # å¦‚æœéœ€è¦è®¡ç®— Hessianï¼Œå¿…é¡» create_graph
            loss.backward(create_graph=update_h)
            
            if update_h:
                # æ‰‹åŠ¨æ³¨å…¥ SCOOP æ¢¯åº¦æ ¡æ­£
                add_hessian_regularization(model, loss, optimizer)
            
            # æ¢¯åº¦è£å‰ª (Adam ä¹Ÿéœ€è¦ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)
            
            optimizer.step()
            
            # å¦‚æœè®¡ç®—å›¾è¢«ä¿ç•™äº†ï¼Œéœ€è¦æ‰‹åŠ¨é‡Šæ”¾ï¼Œé˜²æ­¢æ˜¾å­˜çˆ†ç‚¸
            if update_h:
                del loss
                # æ˜¾å¼æ¸…ç©ºå›¾é€šå¸¸ç”± optimizer.zero_grad å¤„ç†ï¼Œä½†æ‰‹åŠ¨ del æ˜¯å¥½ä¹ æƒ¯
            
            # ğŸ”¥ ä¿®å¤ç‚¹ 3: ç´¯åŠ ç”¨åˆšæ‰å­˜çš„æ•°å€¼ï¼Œè€Œä¸æ˜¯ Tensor
            total_loss += loss_val 
            count += 1
            global_step += 1
            
            curr_lr_display = optimizer.param_groups[0]['lr']
            # è¿™é‡Œä¹Ÿç”¨ total_lossï¼Œé€»è¾‘æ²¡é—®é¢˜
            pbar.set_postfix({'loss': f"{total_loss/count:.4f}", 'lr': f"{curr_lr_display:.6f}"})
            
            if global_step >= args.train_steps: break

        # --- éªŒè¯ ---
        rank, test_loss = evaluate_in_memory(model, args.data_path, n_test=10000, device=device)
        avg_train_loss = total_loss / count
        curr_lr_display = optimizer.param_groups[0]['lr']
        mode_str = "PureAdam" if pure_adam_triggered else "Adam+SCOOP"
        
        print(f"\nEpoch {epoch}: Train {avg_train_loss:.4f} | Test {test_loss:.4f} | Rank {rank} | LR {curr_lr_display:.6e} | Mode {mode_str}")
        
        with open(log_file, "a") as f:
            f.write(f"{epoch},{avg_train_loss:.4f},{test_loss:.4f},{rank},{curr_lr_display:.6e},{mode_str}\n")
            
        if rank < best_rank:
            print(f"â­ New Best Rank! ({best_rank} -> {rank}).")
            best_rank = rank
            torch.save(model.state_dict(), os.path.join(args.checkpoint_dir, "estranet_best_rank.pth"))
            if rank <= 5:
                 torch.save(model.state_dict(), os.path.join(args.checkpoint_dir, f"estranet_top5_rank{rank}.pth"))
        
        # --- å†³ç­–é€»è¾‘ ---
        if rank < 1: # é’ˆå¯¹ 10k æ•°æ®çš„æè‡´è¦æ±‚
            print(f"\nğŸ† VICTORY! Rank {rank} achieved.")
            print(f"   ğŸ›‘ Stopping training.")
            torch.save(model.state_dict(), os.path.join(args.checkpoint_dir, "estranet_FINAL_WINNER.pth"))
            sys.exit(0)
            
        # ğŸ”¥ æ ¸å¿ƒåˆ‡æ¢é€»è¾‘
        if rank < TRIGGER_PURE_ADAM_RANK and not pure_adam_triggered:
            print(f"\nğŸ§  ADAM TAKEOVER: Rank {rank} < {TRIGGER_PURE_ADAM_RANK}")
            print(f"   ğŸš« Disabling Hessian (SCOOP). Switching to Pure AdamW.")
            print(f"   ğŸ“‰ Dropping LR to {PURE_ADAM_LR:.1e} for polishing.")
            
            set_lr(optimizer, PURE_ADAM_LR)
            #pure_adam_triggered = True
            
            # ä¿å­˜åˆ‡æ¢ç‚¹çš„æ¨¡å‹
            torch.save(model.state_dict(), os.path.join(args.checkpoint_dir, "estranet_pre_adam_switch.pth"))

        if global_step >= args.train_steps: break

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # Adam çš„å­¦ä¹ ç‡é€šå¸¸æ¯” SGD å¤§ä¸€ä¸ªæ•°é‡çº§ï¼Œ1e-4 æ˜¯ Transformer/ResNet çš„é»„é‡‘èµ·ç‚¹
    parser.add_argument("--learning_rate", type=float, default=1e-4) 
    parser.add_argument("--data_path", type=str, required=True)
    parser.add_argument("--checkpoint_dir", type=str, default="./checkpoints_hunter_adam")
    parser.add_argument("--result_path", type=str, default="./results_hunter_adam")
    parser.add_argument("--input_length", type=int, default=15000)
    parser.add_argument("--train_batch_size", type=int, default=64)
    parser.add_argument("--train_steps", type=int, default=400000)
    parser.add_argument("--d_model", type=int, default=128)
    parser.add_argument("--n_layer", type=int, default=2)
    parser.add_argument("--n_head", type=int, default=8)
    parser.add_argument("--clip", type=float, default=1.0) # Adam é€šå¸¸è£å‰ªå¾—æ›´ç´§ä¸€ç‚¹
    
    args, unknown = parser.parse_known_args()
    train(args)